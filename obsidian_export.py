#!/usr/bin/env python3
"""
Obsidianæ ‡ç­¾åŒ–å¯¼å‡ºè„šæœ¬
ä¼˜åŒ–ç‰ˆï¼šè‡ªåŠ¨å±‚çº§ç›®å½•ç”Ÿæˆï¼Œæ”¯æŒTagå’ŒCategoryä¸¤ç§æ¨¡å¼
"""

from pathlib import Path
import sys
import re
import subprocess
import json
from datetime import datetime
from collections import defaultdict

# =============================================================================
# é…ç½®
# =============================================================================

# Obsidian vaultè·¯å¾„
VAULT_PATH = Path("./Sth-Matters")

# æ ‡ç­¾å‰ç¼€
TAG_PREFIX = "> Tag:"
CATEGORY_PREFIX = "> Category:"

# è¾“å‡ºæ–‡ä»¶é…ç½®
OUTPUT_FILENAME = "Obsidian_å¯¼å‡º_åˆé›†.epub"
OUTPUT_DIRECTORY = Path("./output")
INDEX_FILENAME = "chapter_index.json"

# =============================================================================
# æ ¸å¿ƒå‡½æ•°
# =============================================================================


def find_all_md_files(vault_path):
    """
    é€’å½’æŸ¥æ‰¾vaultä¸­çš„æ‰€æœ‰.mdæ–‡ä»¶

    Args:
        vault_path (Path): Obsidian vaultçš„è·¯å¾„

    Returns:
        list: æ‰€æœ‰.mdæ–‡ä»¶çš„è·¯å¾„åˆ—è¡¨
    """
    if not vault_path.exists():
        print(f"é”™è¯¯ï¼šè·¯å¾„ {vault_path} ä¸å­˜åœ¨")
        return []

    if not vault_path.is_dir():
        print(f"é”™è¯¯ï¼šè·¯å¾„ {vault_path} ä¸æ˜¯ç›®å½•")
        return []

    md_files = []

    # ä½¿ç”¨é€’å½’globæ¨¡å¼æŸ¥æ‰¾æ‰€æœ‰.mdæ–‡ä»¶
    for md_file in vault_path.rglob("*.md"):
        # è·³è¿‡.obsidianç›®å½•ä¸­çš„æ–‡ä»¶
        if ".obsidian" not in str(md_file):
            md_files.append(md_file)

    return sorted(md_files)


def extract_metadata_from_file(file_path, metadata_type="tag"):
    """
    ä»markdownæ–‡ä»¶ä¸­æå–å…ƒæ•°æ®ï¼ˆæ ‡ç­¾æˆ–åˆ†ç±»ï¼‰

    Args:
        file_path (Path): markdownæ–‡ä»¶è·¯å¾„
        metadata_type (str): "tag" æˆ– "category"

    Returns:
        list: æå–åˆ°çš„å…ƒæ•°æ®åˆ—è¡¨
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # æ ¹æ®ç±»å‹é€‰æ‹©å‰ç¼€
        if metadata_type.lower() == "tag":
            prefix = TAG_PREFIX
            field_name = "æ ‡ç­¾"
        elif metadata_type.lower() == "category":
            prefix = CATEGORY_PREFIX
            field_name = "åˆ†ç±»"
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å…ƒæ•°æ®ç±»å‹: {metadata_type}")

        # æŸ¥æ‰¾å…ƒæ•°æ®è¡Œ
        for line in content.split('\n'):
            line = line.strip()
            if line.startswith(prefix):
                # æå–å†…å®¹ï¼ˆå»æ‰å‰ç¼€ï¼‰
                content_text = line[len(prefix):].strip()

                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰ä»¥#å¼€å¤´çš„é¡¹ç›®
                items = re.findall(r'#[^\s#]+(?:/[^\s#]+)*', content_text)

                return items

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯¹åº”è¡Œï¼Œè¿”å›ç©ºåˆ—è¡¨
        return []

    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        return []


def parse_hierarchy(item):
    """
    è§£æé¡¹ç›®çš„å±‚çº§ç»“æ„

    Args:
        item (str): é¡¹ç›®å­—ç¬¦ä¸²ï¼Œå¦‚ "#1-ä¸ªäººæˆé•¿/1-å†…åœ¨å»ºè®¾/1C-è‡ªå¼º"

    Returns:
        tuple: (å®Œæ•´é¡¹ç›®, å±‚çº§åˆ—è¡¨, æ’åºé”®)
    """
    # å»æ‰å¼€å¤´çš„#
    clean_item = item[1:] if item.startswith('#') else item

    # æŒ‰/åˆ†å‰²è·å–å±‚çº§
    levels = clean_item.split('/')

    # ç”Ÿæˆæ’åºé”®ï¼ˆç”¨äºè‡ªç„¶æ’åºï¼‰
    # æå–æ¯ä¸ªå±‚çº§ä¸­çš„æ•°å­—å’Œå­—æ¯éƒ¨åˆ†è¿›è¡Œæ’åº
    sort_key = []
    for level in levels:
        # æå–æ•°å­—éƒ¨åˆ†å’Œæ–‡å­—éƒ¨åˆ†åˆ†åˆ«å¤„ç†
        import re
        # åŒ¹é…å¼€å¤´çš„æ•°å­—ã€å­—æ¯ã€ä¸­æ–‡ç­‰
        match = re.match(r'^(\d*)([A-Za-z]*)(.*)$', level)
        if match:
            num_part, alpha_part, text_part = match.groups()
            # æ„å»ºæ’åºé”®ï¼š(æ•°å­—éƒ¨åˆ†, å­—æ¯éƒ¨åˆ†, æ–‡å­—éƒ¨åˆ†)
            sort_key.append((
                int(num_part) if num_part else 999999,  # æ²¡æœ‰æ•°å­—çš„æ’åˆ°åé¢
                alpha_part,
                text_part
            ))
        else:
            sort_key.append((999999, '', level))

    return item, levels, tuple(sort_key)


def collect_all_metadata(files_with_metadata):
    """
    æ”¶é›†æ‰€æœ‰å…ƒæ•°æ®å¹¶è¿›è¡Œå±‚çº§è§£æå’Œæ’åº

    Args:
        files_with_metadata (list): åŒ…å«(æ–‡ä»¶è·¯å¾„, å…ƒæ•°æ®åˆ—è¡¨)çš„å…ƒç»„åˆ—è¡¨

    Returns:
        tuple: (æ’åºåçš„å…ƒæ•°æ®åˆ—è¡¨, å…ƒæ•°æ®å±‚çº§å­—å…¸, æ–‡ä»¶å…ƒæ•°æ®æ˜ å°„)
    """
    all_items = set()
    item_hierarchy = {}
    file_item_mapping = {}

    # æ”¶é›†æ‰€æœ‰å…ƒæ•°æ®é¡¹ç›®
    for file_path, items in files_with_metadata:
        if items:
            file_item_mapping[file_path] = items
            for item in items:
                all_items.add(item)

    print(f"æ€»å…±å‘ç° {len(all_items)} ä¸ªä¸åŒçš„é¡¹ç›®")

    # è§£æå±‚çº§
    item_info = []
    for item in all_items:
        full_item, levels, sort_key = parse_hierarchy(item)
        item_info.append((full_item, levels, sort_key))
        item_hierarchy[full_item] = levels

    # æŒ‰æ’åºé”®æ’åº
    item_info.sort(key=lambda x: x[2])
    sorted_items = [info[0] for info in item_info]

    return sorted_items, item_hierarchy, file_item_mapping


def generate_chapter_structure(sorted_items, item_hierarchy, file_item_mapping, metadata_type):
    """
    ç”Ÿæˆç« èŠ‚ç»“æ„

    Args:
        sorted_items (list): æ’åºåçš„å…ƒæ•°æ®åˆ—è¡¨
        item_hierarchy (dict): å…ƒæ•°æ®å±‚çº§å­—å…¸
        file_item_mapping (dict): æ–‡ä»¶å…ƒæ•°æ®æ˜ å°„
        metadata_type (str): "tag" æˆ– "category"

    Returns:
        dict: ç« èŠ‚ç»“æ„å­—å…¸
    """
    chapter_structure = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "metadata_type": metadata_type,
            "total_items": len(sorted_items),
            "total_files": len(file_item_mapping)
        },
        "chapters": []
    }

    # æŒ‰å…ƒæ•°æ®ç»„ç»‡æ–‡ä»¶
    for item in sorted_items:
        levels = item_hierarchy[item]

        # æŸ¥æ‰¾ä½¿ç”¨æ­¤å…ƒæ•°æ®çš„æ–‡ä»¶
        files_with_item = []
        for file_path, file_items in file_item_mapping.items():
            if item in file_items:
                files_with_item.append(str(file_path))

        if files_with_item:  # åªåŒ…å«æœ‰æ–‡ä»¶çš„é¡¹ç›®
            chapter_info = {
                "item": item,
                "levels": levels,
                "level_1": levels[0] if len(levels) > 0 else "",
                "level_2": levels[1] if len(levels) > 1 else "",
                "level_3": levels[2] if len(levels) > 2 else "",
                "level_4": levels[3] if len(levels) > 3 else "",
                "files": files_with_item,
                "file_count": len(files_with_item)
            }
            chapter_structure["chapters"].append(chapter_info)

    return chapter_structure


def save_chapter_index(chapter_structure, output_dir, metadata_type):
    """
    ä¿å­˜ç« èŠ‚ç´¢å¼•åˆ°JSONæ–‡ä»¶

    Args:
        chapter_structure (dict): ç« èŠ‚ç»“æ„
        output_dir (Path): è¾“å‡ºç›®å½•
        metadata_type (str): "tag" æˆ– "category"

    Returns:
        Path: ç´¢å¼•æ–‡ä»¶è·¯å¾„
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    index_filename = f"chapter_index_{metadata_type}.json"
    index_path = output_dir / index_filename

    with open(index_path, 'w', encoding='utf-8') as f:
        json.dump(chapter_structure, f, ensure_ascii=False, indent=2)

    print(f"âœ… ç« èŠ‚ç´¢å¼•å·²ä¿å­˜åˆ°: {index_path}")
    return index_path


def print_chapter_summary(chapter_structure, metadata_type):
    """
    æ‰“å°ç« èŠ‚ç»“æ„æ‘˜è¦

    Args:
        chapter_structure (dict): ç« èŠ‚ç»“æ„
        metadata_type (str): "tag" æˆ– "category"
    """
    print("\n" + "=" * 80)
    print(f"ç« èŠ‚ç»“æ„é¢„è§ˆ (æŒ‰{metadata_type.upper()}åˆ†ç»„)")
    print("=" * 80)

    # æŒ‰å±‚çº§åˆ†ç»„æ˜¾ç¤º
    level1_groups = defaultdict(list)

    for chapter in chapter_structure["chapters"]:
        level1 = chapter["level_1"]
        level1_groups[level1].append(chapter)

    for level1, chapters in level1_groups.items():
        print(f"\nğŸ“ {level1} ({sum(ch['file_count'] for ch in chapters)} ä¸ªæ–‡ä»¶)")

        # æŒ‰äºŒçº§ç›®å½•åˆ†ç»„
        level2_groups = defaultdict(list)
        for chapter in chapters:
            level2 = chapter["level_2"]
            level2_groups[level2].append(chapter)

        for level2, level2_chapters in level2_groups.items():
            if level2:
                print(
                    f"  ğŸ“‚ {level2} ({sum(ch['file_count'] for ch in level2_chapters)} ä¸ªæ–‡ä»¶)")

                for chapter in level2_chapters:
                    level3 = chapter["level_3"]
                    if level3:
                        print(f"    ğŸ“„ {level3} ({chapter['file_count']} ä¸ªæ–‡ä»¶)")
            else:
                # æ²¡æœ‰äºŒçº§ç›®å½•çš„ç›´æ¥æ˜¾ç¤º
                for chapter in level2_chapters:
                    print(f"  ğŸ“„ ç›´æ¥æ–‡ä»¶ ({chapter['file_count']} ä¸ªæ–‡ä»¶)")


def analyze_files_with_metadata(md_files, metadata_type):
    """
    åˆ†ææ‰€æœ‰æ–‡ä»¶å¹¶æå–å…ƒæ•°æ®

    Args:
        md_files (list): markdownæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        metadata_type (str): "tag" æˆ– "category"

    Returns:
        list: åŒ…å«(æ–‡ä»¶è·¯å¾„, å…ƒæ•°æ®åˆ—è¡¨)çš„å…ƒç»„åˆ—è¡¨
    """
    files_with_metadata = []
    field_name = "æ ‡ç­¾" if metadata_type == "tag" else "åˆ†ç±»"

    print(f"æ­£åœ¨åˆ†æ {len(md_files)} ä¸ªæ–‡ä»¶çš„{field_name}...")

    for i, file_path in enumerate(md_files, 1):
        # æ˜¾ç¤ºè¿›åº¦
        if i % 500 == 0 or i == len(md_files):
            print(f"è¿›åº¦: {i}/{len(md_files)}")

        metadata = extract_metadata_from_file(file_path, metadata_type)
        files_with_metadata.append((file_path, metadata))

    return files_with_metadata


def generate_sorted_file_list(chapter_structure):
    """
    æ ¹æ®ç« èŠ‚ç»“æ„ç”Ÿæˆæ’åºåçš„æ–‡ä»¶åˆ—è¡¨

    Args:
        chapter_structure (dict): ç« èŠ‚ç»“æ„

    Returns:
        list: æ’åºåçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    sorted_files = []

    for chapter in chapter_structure["chapters"]:
        for file_path in chapter["files"]:
            sorted_files.append(
                (Path(file_path), [chapter["item"]], [chapter["item"]]))

    return sorted_files


def generate_epub(sorted_files, output_path, metadata_type):
    """
    ä½¿ç”¨Pandocç”ŸæˆEPUBæ–‡ä»¶

    Args:
        sorted_files (list): æ’åºåçš„æ–‡ä»¶åˆ—è¡¨
        output_path (Path): è¾“å‡ºæ–‡ä»¶è·¯å¾„
        metadata_type (str): "tag" æˆ– "category"

    Returns:
        bool: æ˜¯å¦æˆåŠŸç”Ÿæˆ
    """
    if not sorted_files:
        print("é”™è¯¯ï¼šæ²¡æœ‰æ–‡ä»¶éœ€è¦å¯¼å‡º")
        return False

    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # å‡†å¤‡Pandocå‘½ä»¤
        file_paths = [str(file_path) for file_path, _, _ in sorted_files]

        field_name = "æ ‡ç­¾" if metadata_type == "tag" else "åˆ†ç±»"
        title = f"Obsidianå¯¼å‡ºåˆé›†ï¼ˆæŒ‰{field_name}è‡ªåŠ¨å±‚çº§ç‰ˆï¼‰"

        pandoc_cmd = [
            "pandoc",
            # è¾“å…¥æ–‡ä»¶
            *file_paths,
            # è¾“å‡ºæ ¼å¼å’Œæ–‡ä»¶
            "-o", str(output_path),
            # EPUBç›¸å…³é€‰é¡¹
            "--to=epub3",
            "--epub-metadata=metadata.xml" if Path(
                "metadata.xml").exists() else None,
            # ç›®å½•é€‰é¡¹
            "--toc",
            "--toc-depth=3",  # å¢åŠ ç›®å½•æ·±åº¦ä»¥é€‚åº”å±‚çº§ç»“æ„
            # è·³è¿‡YAML frontmatterè§£æï¼Œé¿å…æ ¼å¼é”™è¯¯
            "--from=markdown-yaml_metadata_block",
            # èµ„æºè·¯å¾„ï¼ˆè®©Pandocèƒ½æ‰¾åˆ°å›¾ç‰‡ç­‰èµ„æºï¼‰
            f"--resource-path={VAULT_PATH.absolute()}",
            # æ ‡é¢˜å’Œä½œè€…ä¿¡æ¯
            f"--metadata=title:{title}",
            "--metadata=author:çŸ¥è¯†æ•´ç†è€…",
            f"--metadata=date:{datetime.now().strftime('%Y-%m-%d')}",
            # ä¸­æ–‡æ”¯æŒ
            "--variable=lang:zh-CN",
        ]

        # è¿‡æ»¤æ‰Noneå€¼
        pandoc_cmd = [arg for arg in pandoc_cmd if arg is not None]

        print(f"\næ­£åœ¨ç”ŸæˆEPUBæ–‡ä»¶...")
        print(f"è¾“å‡ºè·¯å¾„: {output_path.absolute()}")
        print(f"å¤„ç†æ–‡ä»¶æ•°: {len(file_paths)}")

        # æ‰§è¡ŒPandocå‘½ä»¤
        result = subprocess.run(
            pandoc_cmd,
            capture_output=True,
            text=True,
            timeout=900  # 15åˆ†é’Ÿè¶…æ—¶
        )

        if result.returncode == 0:
            print("âœ… EPUBæ–‡ä»¶ç”ŸæˆæˆåŠŸ!")
            return True
        else:
            print("âŒ EPUBæ–‡ä»¶ç”Ÿæˆå¤±è´¥!")
            print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
            return False

    except subprocess.TimeoutExpired:
        print("âŒ EPUBç”Ÿæˆè¶…æ—¶!")
        return False
    except Exception as e:
        print(f"âŒ ç”ŸæˆEPUBæ—¶å‡ºé”™: {e}")
        return False


def generate_epub_by_chapters(chapter_structure, output_dir, metadata_type):
    """
    æŒ‰ä¸€çº§ç›®å½•åˆ†åˆ«ç”Ÿæˆå¤šä¸ªEPUBæ–‡ä»¶

    Args:
        chapter_structure (dict): ç« èŠ‚ç»“æ„
        output_dir (Path): è¾“å‡ºç›®å½•
        metadata_type (str): "tag" æˆ– "category"

    Returns:
        list: ç”Ÿæˆçš„EPUBæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    # æŒ‰ä¸€çº§ç›®å½•åˆ†ç»„
    level1_groups = defaultdict(list)

    for chapter in chapter_structure["chapters"]:
        level1 = chapter["level_1"]
        level1_groups[level1].append(chapter)

    generated_files = []
    field_name = "æ ‡ç­¾" if metadata_type == "tag" else "åˆ†ç±»"

    print(f"\nå°†æŒ‰ {len(level1_groups)} ä¸ªä¸€çº§ç›®å½•åˆ†åˆ«ç”ŸæˆEPUBæ–‡ä»¶:")

    for level1, chapters in level1_groups.items():
        # è®¡ç®—æ–‡ä»¶æ€»æ•°
        total_files = sum(len(chapter["files"]) for chapter in chapters)

        print(f"\nğŸ“ æ­£åœ¨å¤„ç†: {level1} ({total_files} ä¸ªæ–‡ä»¶)")

        # ç”Ÿæˆè¯¥åˆ†ç»„çš„æ–‡ä»¶åˆ—è¡¨
        group_files = []
        for chapter in chapters:
            for file_path in chapter["files"]:
                group_files.append(
                    (Path(file_path), [chapter["item"]], [chapter["item"]]))

        # ç”Ÿæˆæ–‡ä»¶åï¼ˆå»æ‰ç‰¹æ®Šå­—ç¬¦ï¼‰
        safe_name = level1.replace(
            "/", "_").replace("\\", "_").replace(":", "_")
        output_filename = f"Obsidian_{metadata_type}_{safe_name}.epub"
        output_path = output_dir / output_filename

        # ç”ŸæˆEPUB
        success = generate_single_epub(
            group_files, output_path, level1, metadata_type)

        if success:
            generated_files.append(output_path)
            print(f"âœ… å·²ç”Ÿæˆ: {output_filename}")
        else:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {output_filename}")

    return generated_files


def generate_single_epub(sorted_files, output_path, category_name, metadata_type):
    """
    ç”Ÿæˆå•ä¸ªEPUBæ–‡ä»¶

    Args:
        sorted_files (list): æ’åºåçš„æ–‡ä»¶åˆ—è¡¨
        output_path (Path): è¾“å‡ºæ–‡ä»¶è·¯å¾„
        category_name (str): åˆ†ç±»åç§°
        metadata_type (str): "tag" æˆ– "category"

    Returns:
        bool: æ˜¯å¦æˆåŠŸç”Ÿæˆ
    """
    if not sorted_files:
        print("é”™è¯¯ï¼šæ²¡æœ‰æ–‡ä»¶éœ€è¦å¯¼å‡º")
        return False

    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # å‡†å¤‡Pandocå‘½ä»¤
        file_paths = [str(file_path) for file_path, _, _ in sorted_files]

        field_name = "æ ‡ç­¾" if metadata_type == "tag" else "åˆ†ç±»"
        title = f"Obsidianå¯¼å‡º - {category_name} (æŒ‰{field_name})"

        pandoc_cmd = [
            "pandoc",
            # è¾“å…¥æ–‡ä»¶
            *file_paths,
            # è¾“å‡ºæ ¼å¼å’Œæ–‡ä»¶
            "-o", str(output_path),
            # EPUBç›¸å…³é€‰é¡¹
            "--to=epub3",
            "--epub-metadata=metadata.xml" if Path(
                "metadata.xml").exists() else None,
            # ç›®å½•é€‰é¡¹
            "--toc",
            "--toc-depth=3",
            # è·³è¿‡YAML frontmatterè§£æï¼Œé¿å…æ ¼å¼é”™è¯¯
            "--from=markdown-yaml_metadata_block",
            # èµ„æºè·¯å¾„ï¼ˆè®©Pandocèƒ½æ‰¾åˆ°å›¾ç‰‡ç­‰èµ„æºï¼‰
            f"--resource-path={VAULT_PATH.absolute()}",
            # æ ‡é¢˜å’Œä½œè€…ä¿¡æ¯
            f"--metadata=title:{title}",
            "--metadata=author:çŸ¥è¯†æ•´ç†è€…",
            f"--metadata=date:{datetime.now().strftime('%Y-%m-%d')}",
            # ä¸­æ–‡æ”¯æŒ
            "--variable=lang:zh-CN",
        ]

        # è¿‡æ»¤æ‰Noneå€¼
        pandoc_cmd = [arg for arg in pandoc_cmd if arg is not None]

        # æ‰§è¡ŒPandocå‘½ä»¤
        result = subprocess.run(
            pandoc_cmd,
            capture_output=True,
            text=True,
            timeout=600  # 10åˆ†é’Ÿè¶…æ—¶ï¼Œå•ä¸ªåˆ†ç±»åº”è¯¥ä¸ä¼šå¤ªå¤§
        )

        if result.returncode == 0:
            return True
        else:
            print(f"ç”Ÿæˆå¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯: {result.stderr}")
            return False

    except subprocess.TimeoutExpired:
        print("ç”Ÿæˆè¶…æ—¶!")
        return False
    except Exception as e:
        print(f"ç”Ÿæˆæ—¶å‡ºé”™: {e}")
        return False


def generate_merged_epub(chapter_structure, output_dir, metadata_type):
    """
    åˆå¹¶æ‰€æœ‰ç« èŠ‚ç”Ÿæˆä¸€ä¸ªå¤§çš„EPUBæ–‡ä»¶

    Args:
        chapter_structure (dict): ç« èŠ‚ç»“æ„
        output_dir (Path): è¾“å‡ºç›®å½•
        metadata_type (str): "tag" æˆ– "category"

    Returns:
        bool: æ˜¯å¦æˆåŠŸç”Ÿæˆ
    """
    field_name = "æ ‡ç­¾" if metadata_type == "tag" else "åˆ†ç±»"
    print(f"\næ­£åœ¨ç”Ÿæˆåˆå¹¶çš„EPUBæ–‡ä»¶ï¼ˆæŒ‰{field_name}ï¼‰...")

    # æ”¶é›†æ‰€æœ‰æ–‡ä»¶ï¼ŒæŒ‰ç« èŠ‚é¡ºåº
    all_files = []
    total_files = 0

    # æŒ‰ä¸€çº§ç›®å½•åˆ†ç»„
    level1_groups = defaultdict(list)
    for chapter in chapter_structure["chapters"]:
        level1 = chapter["level_1"]
        level1_groups[level1].append(chapter)

    # æŒ‰é¡ºåºå¤„ç†æ¯ä¸ªä¸€çº§ç›®å½•
    for level1 in sorted(level1_groups.keys()):
        chapters = level1_groups[level1]
        print(f"ğŸ“ æ·»åŠ ç« èŠ‚: {level1}")

        # æŒ‰ç« èŠ‚é¡ºåºæ·»åŠ æ–‡ä»¶
        for chapter in chapters:
            for file_path in chapter["files"]:
                all_files.append(str(file_path))
                total_files += 1

    print(f"æ€»å…±å°†å¤„ç† {total_files} ä¸ªæ–‡ä»¶")

    # ç”Ÿæˆåˆå¹¶çš„EPUB
    output_filename = f"Obsidian_å®Œæ•´åˆé›†_{metadata_type}.epub"
    output_path = output_dir / output_filename

    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)

        title = f"Obsidianå®Œæ•´çŸ¥è¯†åˆé›†ï¼ˆæŒ‰{field_name}ï¼‰"

        pandoc_cmd = [
            "pandoc",
            # è¾“å…¥æ–‡ä»¶
            *all_files,
            # è¾“å‡ºæ ¼å¼å’Œæ–‡ä»¶
            "-o", str(output_path),
            # EPUBç›¸å…³é€‰é¡¹
            "--to=epub3",
            "--epub-metadata=metadata.xml" if Path(
                "metadata.xml").exists() else None,
            # ç›®å½•é€‰é¡¹
            "--toc",
            "--toc-depth=3",
            # è·³è¿‡YAML frontmatterè§£æï¼Œé¿å…æ ¼å¼é”™è¯¯
            "--from=markdown-yaml_metadata_block",
            # èµ„æºè·¯å¾„ï¼ˆè®©Pandocèƒ½æ‰¾åˆ°å›¾ç‰‡ç­‰èµ„æºï¼‰
            f"--resource-path={VAULT_PATH.absolute()}",
            # æ ‡é¢˜å’Œä½œè€…ä¿¡æ¯
            f"--metadata=title:{title}",
            "--metadata=author:çŸ¥è¯†æ•´ç†è€…",
            f"--metadata=date:{datetime.now().strftime('%Y-%m-%d')}",
            # ä¸­æ–‡æ”¯æŒ
            "--variable=lang:zh-CN",
        ]

        # è¿‡æ»¤æ‰Noneå€¼
        pandoc_cmd = [arg for arg in pandoc_cmd if arg is not None]

        print(f"æ­£åœ¨ç”Ÿæˆåˆå¹¶EPUBæ–‡ä»¶ï¼ˆè¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰...")
        print(f"è¾“å‡ºè·¯å¾„: {output_path.absolute()}")

        # æ‰§è¡ŒPandocå‘½ä»¤ï¼Œä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´
        result = subprocess.run(
            pandoc_cmd,
            capture_output=True,
            text=True,
            timeout=1800  # 30åˆ†é’Ÿè¶…æ—¶
        )

        if result.returncode == 0:
            print("âœ… åˆå¹¶EPUBæ–‡ä»¶ç”ŸæˆæˆåŠŸ!")

            # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
            if output_path.exists():
                file_size = output_path.stat().st_size
                size_mb = file_size / (1024 * 1024)
                print(f"ğŸ“ åˆå¹¶æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")
                print(f"ğŸ“Š åŒ…å«æ–‡ä»¶æ•°: {total_files}")

            return True
        else:
            print("âŒ åˆå¹¶EPUBæ–‡ä»¶ç”Ÿæˆå¤±è´¥!")
            print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
            return False

    except subprocess.TimeoutExpired:
        print("âŒ åˆå¹¶EPUBç”Ÿæˆè¶…æ—¶!")
        return False
    except Exception as e:
        print(f"âŒ ç”Ÿæˆåˆå¹¶EPUBæ—¶å‡ºé”™: {e}")
        return False


# =============================================================================
# ä¸»ç¨‹åº
# =============================================================================

def main():
    """ä¸»ç¨‹åº"""
    print("=" * 80)
    print("Obsidianæ ‡ç­¾åŒ–å¯¼å‡ºè„šæœ¬ - è‡ªåŠ¨å±‚çº§ç›®å½•ç”Ÿæˆç‰ˆï¼ˆæ”¯æŒTag/Categoryï¼‰")
    print("=" * 80)

    print(f"æ­£åœ¨æ‰«æç›®å½•: {VAULT_PATH.absolute()}")

    # ç¬¬ä¸€æ­¥ï¼šæŸ¥æ‰¾æ‰€æœ‰.mdæ–‡ä»¶
    md_files = find_all_md_files(VAULT_PATH)
    print(f"æ‰¾åˆ° {len(md_files)} ä¸ªmarkdownæ–‡ä»¶")

    # ç¬¬äºŒæ­¥ï¼šé€‰æ‹©å¤„ç†æ¨¡å¼
    print(f"\nè¯·é€‰æ‹©å¤„ç†æ¨¡å¼:")
    print(f"1. æŒ‰æ ‡ç­¾ (Tag) å¤„ç†")
    print(f"2. æŒ‰åˆ†ç±» (Category) å¤„ç†")

    while True:
        mode_choice = input("è¯·é€‰æ‹© (1/2): ").strip()
        if mode_choice in ['1', '2']:
            break
        print("è¯·è¾“å…¥æœ‰æ•ˆé€‰æ‹©: 1 æˆ– 2")

    metadata_type = "tag" if mode_choice == '1' else "category"
    field_name = "æ ‡ç­¾" if metadata_type == "tag" else "åˆ†ç±»"

    print(f"\nå·²é€‰æ‹©: æŒ‰{field_name}å¤„ç†")

    # ç¬¬ä¸‰æ­¥ï¼šæå–å…ƒæ•°æ®
    files_with_metadata = analyze_files_with_metadata(md_files, metadata_type)

    # ç¬¬å››æ­¥ï¼šæ”¶é›†å¹¶æ’åºæ‰€æœ‰å…ƒæ•°æ®
    print(f"\næ­£åœ¨æ”¶é›†å’Œåˆ†æ{field_name}...")
    sorted_items, item_hierarchy, file_item_mapping = collect_all_metadata(
        files_with_metadata)

    # ç¬¬äº”æ­¥ï¼šç”Ÿæˆç« èŠ‚ç»“æ„
    print(f"\næ­£åœ¨ç”Ÿæˆç« èŠ‚ç»“æ„...")
    chapter_structure = generate_chapter_structure(
        sorted_items, item_hierarchy, file_item_mapping, metadata_type)

    # ç¬¬å…­æ­¥ï¼šä¿å­˜ç´¢å¼•æ–‡ä»¶
    index_path = save_chapter_index(
        chapter_structure, OUTPUT_DIRECTORY, metadata_type)

    # ç¬¬ä¸ƒæ­¥ï¼šæ˜¾ç¤ºç« èŠ‚ç»“æ„é¢„è§ˆ
    print_chapter_summary(chapter_structure, metadata_type)

    # ç¬¬å…«æ­¥ï¼šç”Ÿæˆæ’åºæ–‡ä»¶åˆ—è¡¨
    sorted_files = generate_sorted_file_list(chapter_structure)

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 80)
    print("å¯¼å‡ºç»Ÿè®¡:")
    print("=" * 80)
    print(f"åŸå§‹æ–‡ä»¶æ€»æ•°: {len(md_files)}")
    print(f"æœ‰{field_name}çš„æ–‡ä»¶: {len(file_item_mapping)}")
    print(f"åŒ…å«çš„{field_name}æ•°: {len(sorted_items)}")
    print(f"å°†å¯¼å‡ºçš„æ–‡ä»¶: {len(sorted_files)}")

    # è¯¢é—®EPUBç”Ÿæˆæ–¹å¼
    print(f"\né€‰æ‹©EPUBç”Ÿæˆæ–¹å¼:")
    print(f"1. æŒ‰ä¸€çº§ç›®å½•åˆ†åˆ«ç”Ÿæˆå¤šä¸ªEPUBæ–‡ä»¶ï¼ˆæ¨èï¼Œé¿å…è¶…æ—¶ï¼‰")
    print(f"2. ç”Ÿæˆå•ä¸ªå®Œæ•´EPUBæ–‡ä»¶ï¼ˆå¯èƒ½è¶…æ—¶ï¼‰")
    print(f"3. åˆå¹¶æ‰€æœ‰ç« èŠ‚ç”Ÿæˆä¸€ä¸ªå¤§çš„EPUBæ–‡ä»¶")
    print(f"4. è·³è¿‡EPUBç”Ÿæˆ")

    while True:
        choice = input("è¯·é€‰æ‹© (1/2/3/4): ").strip()
        if choice in ['1', '2', '3', '4']:
            break
        print("è¯·è¾“å…¥æœ‰æ•ˆé€‰æ‹©: 1ã€2ã€3 æˆ– 4")

    if choice == '1':
        # æŒ‰ç« èŠ‚åˆ†åˆ«ç”ŸæˆEPUB
        print(f"\næ­£åœ¨æŒ‰ä¸€çº§ç›®å½•åˆ†åˆ«ç”ŸæˆEPUBæ–‡ä»¶...")
        generated_files = generate_epub_by_chapters(
            chapter_structure, OUTPUT_DIRECTORY, metadata_type)

        if generated_files:
            print(f"\nğŸ‰ åˆ†ç« èŠ‚å¯¼å‡ºæˆåŠŸå®Œæˆ!")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIRECTORY.absolute()}")
            print(f"ğŸ“ ç´¢å¼•æ–‡ä»¶: {index_path.absolute()}")
            print(f"ğŸ“Š ç”Ÿæˆçš„EPUBæ–‡ä»¶: {len(generated_files)} ä¸ª")

            total_size = 0
            for epub_file in generated_files:
                if epub_file.exists():
                    file_size = epub_file.stat().st_size
                    size_mb = file_size / (1024 * 1024)
                    total_size += file_size
                    print(f"   ğŸ“„ {epub_file.name}: {size_mb:.2f} MB")

            total_size_mb = total_size / (1024 * 1024)
            print(f"ğŸ“ æ€»å¤§å°: {total_size_mb:.2f} MB")
        else:
            print(f"\nâŒ åˆ†ç« èŠ‚å¯¼å‡ºå¤±è´¥")

    elif choice == '2':
        # ç”Ÿæˆå•ä¸ªå®Œæ•´EPUB
        print(f"\næ­£åœ¨ç”Ÿæˆå®Œæ•´EPUBæ–‡ä»¶...")
        output_filename = f"Obsidian_å¯¼å‡º_åˆé›†_{metadata_type}.epub"
        output_path = OUTPUT_DIRECTORY / output_filename
        success = generate_epub(sorted_files, output_path, metadata_type)

        if success:
            print(f"\nğŸ‰ å®Œæ•´å¯¼å‡ºæˆåŠŸå®Œæˆ!")
            print(f"ğŸ“ EPUBæ–‡ä»¶: {output_path.absolute()}")
            print(f"ğŸ“ ç´¢å¼•æ–‡ä»¶: {index_path.absolute()}")
            print(f"ğŸ“Š åŒ…å«æ–‡ä»¶æ•°: {len(sorted_files)}")

            # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
            if output_path.exists():
                file_size = output_path.stat().st_size
                size_mb = file_size / (1024 * 1024)
                print(f"ğŸ“ EPUBå¤§å°: {size_mb:.2f} MB")
        else:
            print(f"\nâŒ å®Œæ•´å¯¼å‡ºå¤±è´¥ï¼Œå»ºè®®å°è¯•åˆ†ç« èŠ‚ç”Ÿæˆ")
    elif choice == '3':
        # åˆå¹¶æ‰€æœ‰ç« èŠ‚ç”Ÿæˆä¸€ä¸ªå¤§çš„EPUB
        success = generate_merged_epub(
            chapter_structure, OUTPUT_DIRECTORY, metadata_type)
        if success:
            print(f"\nğŸ“ ç´¢å¼•æ–‡ä»¶: {index_path.absolute()}")
        else:
            print(f"\nâŒ åˆå¹¶å¯¼å‡ºå¤±è´¥")
    else:
        print(f"\nâœ… ç« èŠ‚ç´¢å¼•ç”Ÿæˆå®Œæˆï¼ŒEPUBç”Ÿæˆå·²è·³è¿‡")
        print(f"ğŸ“ ç´¢å¼•æ–‡ä»¶: {index_path.absolute()}")


if __name__ == "__main__":
    main()
